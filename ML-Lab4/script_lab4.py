"""
Introduction to Machine Learning

Lab 4: Non-linear regression

TODO: Add your information here.
    IMPORTANT: Please ensure this script
    (1) Run script_lab4.py on Python >=3.6;
    (2) No errors;
    (3) Finish in tolerable time on a single CPU (e.g., <=10 mins);
Student name(s): 张宇尧
Student ID(s): 2020201710
"""

import copy
from tkinter import Y
import numpy as np
import matplotlib.pyplot as plt
# don't add any other packages


# data simulator and testing function (Don't change them)
def polynomial_data_simulator(n_train: int = 50,
                              n_test: int = 10,
                              order: int = 5,
                              v_noise: float = 1,
                              r_seed: int = 42) -> dict:
    """
    Simulate the training and testing data generated by a polynomial model
    :param n_train: the number of training data
    :param n_test: the number of testing data
    :param order: the order of the polynomial function
    :param v_noise: the variance of noise
    :param r_seed: the random seed
    :return:
        a dictionary containing training set, testing set, and the ground truth parameters
    """
    x_train = 4 * (np.random.RandomState(r_seed).rand(n_train) - 0.5)
    X_train = np.array([x_train ** d for d in range(order)]).T
    x_test = 5 * (np.random.RandomState(r_seed).rand(n_test) - 0.5)
    X_test = np.array([x_test ** d for d in range(order)]).T
    weights = np.random.RandomState(r_seed).randn(order, 1)
    y_train = X_train @ weights + v_noise * np.random.RandomState(r_seed).randn(n_train, 1)
    y_test = X_test @ weights + v_noise * np.random.RandomState(r_seed).randn(n_test, 1)
    data = {'train': [np.expand_dims(x_train, axis=1), y_train],
            'test': [np.expand_dims(x_test, axis=1), y_test],
            'real': weights}
    return data


def visualization_curves(weights: np.ndarray, label: str, curve_type: str):
    landmarks = 5 * (np.arange(0, 100) / 100 - 0.5)
    order = weights.shape[0]
    curve = np.array([landmarks ** d for d in range(order)]).T @ weights
    plt.plot(landmarks, curve, curve_type, label=label)


def visualization_points(x: np.ndarray, y: np.ndarray, label: str, point_type: str):
    plt.plot(x, y, point_type, label=label)


def visualization_kernel_curves(alpha: np.ndarray, x_train: np.ndarray,
                                k_type: str, h: float, label: str, curve_type: str, normalize: bool = False):
    landmarks = 5 * (np.arange(0, 100) / 100 - 0.5)
    kappa = kernel(x=x_train, y=np.expand_dims(landmarks, axis=1), k_type=k_type, bandwidth=h)
    if normalize:
        curve = (kappa / np.sum(kappa + 1e-10, axis=1, keepdims=True)) @ alpha
    else:
        curve = kappa @ alpha
    plt.plot(landmarks, curve, curve_type, label=label)


def mse(x: np.ndarray, x_est: np.ndarray) -> float:
    return np.sum((x - x_est) ** 2) / x.shape[0]


# Task 1: Implement typical valid kernel functions and the Nadaraya-Watson estimator
def kernel(x: np.ndarray, y: np.ndarray = None, k_type: str = 'rbf', bandwidth: float = 0.1) -> np.ndarray:
    """
    Implement four kinds of typical kernel functions
    1) RBF kernel: k(x, y) = exp(-||x - y||_2^2 / bandwidth)
    2) 'Gate' kernel: k(x, y) = 1/bandwidth if ||x - y||_1 <= bandwidth, = 0 otherwise
    3) 'Triangle' kernel: k(x, y) = (bandwidth - ||x - y||_1) if ||x - y||_1 <= bandwidth, = 0 otherwise
    4) Linear kernel: k(x, y) = <x, y>
    :param x: a set of samples with size (N, D), where N is the number of samples, D is the dimension of features
    :param y: a set of samples with size (M, D), where M is the number of samples. this input can be None
    :param k_type: the type of kernels, including 'rbf', 'gate', 'triangle', 'linear'
    :param bandwidth: the hyperparameter controlling the width of rbf/gate/triangle kernels
    :return:
        if y = None, return a matrix with size (N, N)
        otherwise, return a matrix with size (M, N)
    """
    if y is None:
        y = copy.deepcopy(x)
    # TODO: Change the code below and implement the four kinds of kernel functions mentioned in the above comments.
    
    K = np.zeros((y.shape[0], x.shape[0]))
    if k_type == 'rbf' :
        for i in range(y.shape[0]) :
            for j in range(x.shape[0]) :
                K[i, j] = np.exp(-(np.sum((x[j].reshape(-1, 1) - y[i].reshape(-1, 1)) ** 2, axis=0) / bandwidth))
                
    elif k_type == 'gate' :
        for i in range(y.shape[0]) :
            for j in range(x.shape[0]) :
                if np.linalg.norm(x[j].reshape(-1, 1) - y[i].reshape(-1, 1), ord = 1) <= bandwidth :
                    K[i, j] = 1 / bandwidth
                else :
                    K[i, j] = 0
                    
    elif k_type == 'triangle' :
        for i in range(y.shape[0]) :
            for j in range(x.shape[0]) :
                k_ij = np.linalg.norm(x[j].reshape(-1, 1) - y[i].reshape(-1, 1), ord = 1)
                if k_ij <= bandwidth :
                    K[i, j] = bandwidth - k_ij
                else :
                    K[i, j] = 0
                    
    elif k_type == 'linear':
        for i in range(y.shape[0]) :
            for j in range(x.shape[0]) :
                K[i, j] = np.inner(x[j].reshape(1, -1), y[i].reshape(1, -1))
    
    return K

def nadaraya_watson_estimator(x_test: np.ndarray, x_train: np.ndarray, y_train: np.ndarray, k_type: str, h: float) -> np.ndarray:
    """
    Implement the Nadaraya-Watson estimator:

    y_test = sum_{n=1}^{N} kappa_h(x_test - x_n) / sum_{i=1}^{N} kappa_h(x)

    :param x_test: the input data of testing set, with size (M, D)
    :param x_train: the input data in the training set, with size (N, D)
    :param y_train: the label/output data in the training set, with size (N, 1)
    :param k_type: the type of kernel, default is 'rbf', and other options include 'gate', 'triangle', 'linear'
    :param h: the hyperparameter controlling the width of rbf/gate/triangle kernels
    :return:
        the estimation of y_test with size (M, 1)
    """
    # TODO: Change the code below and implement the NW-estimator based on specific kernel functions
    y_test = np.zeros((x_test.shape[0], 1))
    K = kernel(x_train,
               x_test,
               k_type = k_type,
               bandwidth=h)
    for i in range(x_test.shape[0]) :
        if np.sum(K[i]) == 0 :
            y_test[i] = np.inner((K[i]), y_train.reshape(1, -1))
        else :
            y_test[i] = np.inner((K[i]) / np.sum(K[i]).reshape(1, -1), y_train.reshape(1, -1))
    return y_test


# Task 2: Implement the training and the testing method of Kernel Ridge Regression method.
def training_krr(x_train: np.ndarray, y_train: np.ndarray, k_type: str, h: float, tau: float = 0.1) -> np.ndarray:
    """
    The training method of kernel ridge regression (KRR)

    min_{a} ||y - Ka||_2^2 + tau * a^T K a

    :param x_train: the input data in the training set, with size (N, D)
    :param y_train: the label/output data in the training set, with size (N, 1)
    :param k_type: the type of kernel, default is 'rbf', and other options include 'gate', 'triangle', and 'linear'
    :param h: the hyperparameter controlling the width of rbf/gate/triangle kernels
    :param tau: the hyperparameter controlloing the significance of the regularizer
    :return:
        the weights associated with the training data, with size (N, 1)
    """
    # TODO: Change the code below and implement the closed form solution of kernel ridge regression
    #   Hint: Use as few computations as possible
    alpha = np.zeros_like(y_train)
    K = kernel(x_train,
               x_train,
               k_type = k_type,
               bandwidth = h)
    # alpha = np.linalg.inv(K + tau * np.identity(x_train.shape[0])) @ y_train these are equal
    alpha = np.linalg.pinv(K.T @ K + tau * K.T) @ K.T @ y_train
    return alpha


def testing_krr(x_test: np.ndarray, x_train: np.ndarray, alpha: np.ndarray, k_type: str, h: float) -> np.ndarray:
    """
    Testing a learned kernel ridge regression model
    :param x_test: the input data of testing set, with size (M, D)
    :param x_train: the input data in the training set, with size (N, D)
    :param alpha: the learned KRR model, with size (N, 1)
    :param k_type: the type of kernel, default is 'rbf', and other options include 'gate', 'triangle', and 'linear'
    :param h: the hyperparameter controlling the width of rbf/gate/triangle kernels
    :return:
        y_test, the estimated output with size (M, 1)
    """
    # TODO: Change the code below and implement the prediction method of kernel ridge regression
    y_test = np.zeros_like(x_test)
    K = kernel(x_train,
               x_test,
               bandwidth = h,
               k_type = k_type)
    y_test = K @ alpha
    return y_test 


# Task 3: Try to learn the KRR model via the stochastic gradient descent (sgd) algorithm,
# and CHECK WHETHER it works or not:) and try to make it work as much as possible
def training_krr_sgd(x_train: np.ndarray,
                     y_train: np.ndarray,
                     k_type: str,
                     h: float,
                     tau: float = 0.1,
                     epoches: int = 20,
                     batch_size: int = 10,
                     lr: float = 1e-3,
                     r_seed: int = 1) -> np.ndarray:
    """
    The stochastic gradient descent method of kernel ridge regression.

    :param x_train: the input data in the training set, with size (N, D)
    :param y_train: the label/output data in the training set, with size (N, 1)
    :param k_type: the type of kernel, default is 'rbf', and other options include 'gate', 'triangle', and 'linear'
    :param h: the hyperparameter controlling the width of rbf/gate/triangle kernels
    :param tau: the hyperparameter controlling the significance of the regularizer
    :param epoch: the number of epochs
    :param batch_size: the batch size for sgd
    :param lr: the learning rate
    :param r_seed: random seed to initialize model parameters
    :return:
        the weights associated with the training data, with size (N, 1)
    """
    # TODO: Change the code below and try to implement a SGD algorithm for kernel ridge regression
    #  Hint 1: Think about the rationality of this SGD method based on your implementation and the corresponding results
    #  Hint 2: If your result is not good, try to adjust the hyperparameters (lr, tau, epoch) by yourself.
    alpha = np.random.RandomState(r_seed).randn(x_train.shape[0], 1)
    K = kernel(x_train,
               x_train,
               k_type = k_type,
               bandwidth = h)
    for epoch in range(epoches) :
        index = [i for i in range(x_train.shape[0])]
        np.random.RandomState(r_seed).shuffle(index)
        K = K[index]
        y_train = y_train[index]
        for batch in range(int(x_train.shape[0] / batch_size)) :
            K_batch = K[batch * 10: batch * 10 + batch_size]
            y_batch = y_train[batch * 10: batch * 10 + batch_size]
            avg_grad = (2 * K_batch.T @ (K_batch @ alpha - y_batch) + 2 * tau * (K @ alpha)) / batch_size
            alpha = alpha - lr * avg_grad
    return alpha


# Testing script
if __name__ == '__main__':
    data = polynomial_data_simulator()
    kernel_types = ['rbf', 'gate', 'triangle', 'linear']
    bandwidths = [0.1, 1, 2, 5]
    for k in range(len(kernel_types)):
        for i in range(len(bandwidths)):
            ker_type = kernel_types[k]
            bw = bandwidths[i]
            y_est0 = nadaraya_watson_estimator(x_test=data['test'][0],
                                               x_train=data['train'][0],
                                               y_train=data['train'][1],
                                               k_type=ker_type, h=bw)
            alpha1 = training_krr(x_train=data['train'][0], y_train=data['train'][1], k_type=ker_type, h=bw)
            y_est1 = testing_krr(x_test=data['test'][0], x_train=data['train'][0], alpha=alpha1, k_type=ker_type, h=bw)
            alpha2 = training_krr_sgd(x_train=data['train'][0], y_train=data['train'][1], k_type=ker_type, h=bw)
            y_est2 = testing_krr(x_test=data['test'][0], x_train=data['train'][0], alpha=alpha2, k_type=ker_type, h=bw)

            mse0 = mse(data['test'][1], y_est0)
            mse1 = mse(data['test'][1], y_est1)
            mse2 = mse(data['test'][1], y_est2)

            setting = 'Kernel={}, Bandwidth={:.2f}'.format(
                ker_type, bw)
            result = 'NWKR: MSE={:.4f}, KRR: MSE={:.4f}, KRR-sgd: MSE={:.4f}'.format(
                mse0, mse1, mse2)
            print(setting + ' ' + result)

            plt.figure()
            visualization_points(x=data['train'][0], y=data['train'][1][:, 0], point_type='bx', label='training data')
            visualization_points(x=data['test'][0], y=data['test'][1][:, 0], point_type='kx', label='testing data')
            visualization_curves(weights=data['real'], label='ground truth', curve_type='r-')
            visualization_kernel_curves(alpha=data['train'][1], x_train=data['train'][0],
                                        k_type=ker_type, h=bw, label='nwk', curve_type='g-', normalize=True)
            visualization_kernel_curves(alpha=alpha1, x_train=data['train'][0],
                                        k_type=ker_type, h=bw, label='krr', curve_type='b-')
            visualization_kernel_curves(alpha=alpha2, x_train=data['train'][0],
                                        k_type=ker_type, h=bw, label='krr-sgd', curve_type='k:')
            plt.title(result)
            plt.xlabel(setting)
            plt.legend()
            plt.savefig('result_{}_{}.png'.format(k, i))
            plt.close('all')
