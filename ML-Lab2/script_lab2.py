"""
Introduction to Machine Learning

Lab 2: Polynomial regression

TODO: Add your information here.
    IMPORTANT: Please ensure this script
    (1) Run script_lab2.py on Python >=3.6;
    (2) No errors;
    (3) Finish in tolerable time on a single CPU (e.g., <=10 mins);
Student name(s): 张宇尧
Student ID(s): 2020201710
"""

import numpy as np
import matplotlib.pyplot as plt
from pyrsistent import inc
from torch import norm
# don't add any other packages


# data simulator and testing function (Don't change them)
def polynomial_data_simulator(n_train: int = 50,
                              n_test: int = 10,
                              order: int = 5,
                              v_noise: float = 2,
                              r_seed: int = 42) -> dict:
    """
    Simulate the training and testing data generated by a polynomial model
    :param n_train: the number of training data
    :param n_test: the number of testing data
    :param order: the order of the polynomial function
    :param v_noise: the variance of noise
    :param r_seed: the random seed
    :return:
        a dictionary containing training set, testing set, and the ground truth parameters
    """
    x_train = 4 * (np.random.RandomState(r_seed).rand(n_train) - 0.5)
    X_train = np.array([x_train ** d for d in range(order)]).T
    x_test = 5 * (np.random.RandomState(r_seed).rand(n_test) - 0.5)
    X_test = np.array([x_test ** d for d in range(order)]).T
    weights = np.random.RandomState(r_seed).randn(order, 1)
    y_train = X_train @ weights + v_noise * np.random.RandomState(r_seed).randn(n_train, 1)
    y_test = X_test @ weights + v_noise * np.random.RandomState(r_seed).randn(n_test, 1)
    data = {'train': [x_train, y_train],
            'test': [x_test, y_test],
            'real': weights}
    return data


def visualization_curves(weights: np.ndarray, label: str, curve_type: str):
    landmarks = 5 * (np.arange(0, 100) / 100 - 0.5)
    order = weights.shape[0]
    curve = np.array([landmarks ** d for d in range(order)]).T @ weights
    plt.plot(landmarks, curve, curve_type, label=label)


def visualization_points(x: np.ndarray, y: np.ndarray, label: str, point_type: str):
    plt.plot(x, y, point_type, label=label)


def testing(x: np.ndarray, y: np.ndarray, weights: np.ndarray) -> float:
    """
    Compute the MSE of regression based on current model
    :param x: testing data with size (N, )
    :param y: testing label with size (N, 1)
    :param weights: model parameter with size (D, 1)
    :return:
        MSE
    """
    order = weights.shape[0]
    x_test = np.array([x ** d for d in range(order)]).T
    y_est = x_test @ weights
    return np.sum((y - y_est) ** 2) / x.shape[0]


# Task 1: Implement the training function of a polynomial regression model,
# which derives the closed form solution directly
def training(x: np.ndarray, y: np.ndarray, order: int) -> np.ndarray:
    """
    The training function of polynomial regression model
    :param x: input data with size (N,)
    :param y: labels of data with size (N, 1)
    :param order: the hyper-parameter determining the order of the polynomial function
    :return:
        a weight vector with size (order, 1)
    """
    # TODO: Change the code below and learn the regression model
    x_vander = np.vander(x, order, increasing = True)
    w = np.matmul(np.matmul(np.linalg.inv(np.matmul(x_vander.T, x_vander)), x_vander.T), y)
    return w


# Task 2: Implement the training function of a polynomial regression model,
# which implements the stochastic gradient descent (sgd) algorithm.
def training_sgd(x: np.ndarray,
                 y: np.ndarray,
                 order: int,
                 epoch: int = 20,
                 batch_size: int = 10,
                 lr: float = 1e-4,
                 r_seed: int = 1) -> np.ndarray :
    """
    The stochastic gradient descent method
    :param x: input data with size (N,)
    :param y: labels of data with size (N, 1)
    :param order: the hyper-parameter determining the order of the polynomial function
    :param epoch: the number of epochs
    :param batch_size: the batch size for sgd
    :param lr: the learning rate
    :param r_seed: random seed
    :return:
        a weight vector with size (order, 1)
    """
    # TODO: Change the code below and learn the regression model
    #  Hint: 1) Initialize weight vector randomly;
    #        2) For each epoch, sampling batches from (x, y) randomly and compute the gradient of loss function
    #        3) Update w = w - lr * grad
    np.random.seed(r_seed)
    w = np.random.rand(order, 1) 
    for ep in range(epoch) :
        rand_index = np.random.randint(0, x.shape[0], batch_size)
        samples_x = x[rand_index]
        samples_y = y[rand_index]
        samples_x = np.vander(samples_x, order, increasing = True)
        grad = 0.5 * np.matmul(samples_x.T, np.matmul(samples_x, w) - samples_y) # Power point review!!
        w = w - lr * grad
    return w


# Task 3: Implement various data normalization method
def data_normalization(x: np.ndarray, order: int, method: str = 'L2') -> np.ndarray :
    """
    implement various data_normalization method
    :param x: input data with size (N, )
    :param order: the hyper-parameter determining the order of the polynomial function
    :param method: 'L2': ensure the L2-norm of each column of data matrix is 1
                   'L1': ensure the L1-norm of each column of data matrix is 1
                   'Max': ensure the Infinity-norm of each column of data matrix is 1
    :return:
        A data matrix with size (N, order), each column is normalized according to the method we set
        A scaling vector containing the norms of original columns, with size (order, )
    """
    # TODO: Change the code below and achieve various data normalization methods
    x1 = np.vander(x, order, increasing = True)
    if method == 'L2' :
        norm = np.linalg.norm(x1, ord=2, axis=0, keepdims=True)
        x_normlized = x1 / norm
        norm = (1 / norm).T
        return x_normlized, norm 
    elif method == 'L1' :
        norm = np.linalg.norm(x1, ord=1, axis=0, keepdims=True)
        x_normlized = x1 / norm
        norm = (1 / norm).T
        return x_normlized, norm 
    else :
        norm = np.linalg.norm(x1, ord=np.inf, axis=0, keepdims=True)
        x_normlized = x1 / norm
        norm = (1 / norm).T
        return x_normlized, norm 
    


# Task 4: Implement the SGD training method based on normalized data.
def training_sgd_normalized(x: np.ndarray,
                            y: np.ndarray,
                            order: int,
                            epoch: int = 20,
                            batch_size: int = 10,
                            lr: float = 1e-4,
                            method: str = 'L2',
                            r_seed: int = 1)  -> np.ndarray :
    """
        The stochastic gradient descent method
        :param x: input data with size (N,)
        :param y: labels of data with size (N, 1)
        :param order: the hyper-parameter determining the order of the polynomial function
        :param epoch: the number of epochs
        :param batch_size: the batch size for sgd
        :param lr: the learning rate
        :param method: the data normalization method (L2, L1, or Max)
        :param r_seed: random seed
        :return:
            A weight vector with size (order, 1)
            A scaling vector containing the norms of original columns, with size (order, )
    """
    # TODO: Change the code below and learn the regression model
    #  Hint: combine the above function "data_normalization" with "training_sgd"
    np.random.seed(r_seed)
    w = np.random.rand(order, 1)
    _ , norm = data_normalization(x, order=order, method=method)
    for ep in range(epoch) : 
        rand_index = np.random.randint(0, x.shape[0], batch_size)
        samples_x = x[rand_index]
        samples_x , _ = data_normalization(samples_x, order=order, method=method)
        samples_y = y[rand_index]
        grad = 0.5 * np.matmul(samples_x.T, np.matmul(samples_x, w) - samples_y)
        w = w - lr * grad
    return w, norm

# Task 5: Testing the model trained on the normalized data
def testing_normalized(x: np.ndarray, y: np.ndarray, weights: np.ndarray, scaling: np.ndarray) -> float:
    """
    Compute the MSE of regression based on current model
    :param x: testing data with size (N, )
    :param y: testing label with size (N, 1)
    :param weights: model parameter with size (D, 1)
    :param scaling: A scaling vector containing the norms of original columns, with size (order, )
    :return:
        MSE
    """
    # TODO: Change the code below and testing the model trained on the normalized data
    #  Hint: modify the "testing" function with the help of the scaling vector you derived by training_sgd_normalized.
    order = weights.shape[0]
    x_test = np.array([x ** d for d in range(order)]).T
    x_test = x_test * scaling.T 
    y_est = x_test @ weights
    return np.sum((y - y_est) ** 2) / x.shape[0]

# np.sum((y - y_est) ** 2) / x.shape[0]
# Testing script
if __name__ == '__main__':
    data = polynomial_data_simulator()
    orders = [3, 5, 7, 9] 
    for i in range(len(orders)):
        w_est1 = training(x=data['train'][0], y=data['train'][1], order=orders[i])
        w_est2 = training_sgd(x=data['train'][0], y=data['train'][1], order=orders[i])
        data_normalization(x = data['train'][0], order = orders[i])
        w_est3, scales = training_sgd_normalized(x=data['train'][0], y=data['train'][1], order=orders[i])
        mse1 = testing(x=data['test'][0], y=data['test'][1], weights=w_est1)
        mse2 = testing(x=data['test'][0], y=data['test'][1], weights=w_est2)
        mse3 = testing_normalized(x=data['test'][0], y=data['test'][1], weights=w_est3, scaling=scales)
        print('Order={}, mse1={:.4f}, mse2={:.4f}, mse3={:.4f}'.format(orders[i], mse1, mse2, mse3))
        plt.figure()
        visualization_points(x=data['train'][0], y=data['train'][1][:, 0], point_type='bx', label='training data')
        visualization_points(x=data['test'][0], y=data['test'][1][:, 0], point_type='kx', label='testing data')
        visualization_curves(weights=data['real'], label='ground truth', curve_type='r-')
        visualization_curves(weights=w_est2, label='estimator 2', curve_type='g--')
        plt.savefig('Order{}.png'.format(orders[i]))
        plt.close('all')
